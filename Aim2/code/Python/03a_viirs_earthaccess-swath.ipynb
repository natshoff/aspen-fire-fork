{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1ec03ae-2946-4baf-a5f7-f722a615f19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready !\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Downloading VIIRS Active Fire Detections (AFD) with 'earthaccess' python API\n",
    "\n",
    "Searches and accesses VIIRS AFD for fire perimeters.\n",
    "\n",
    "VIIRS/NPP Active Fires 6-Min L2 Swath 375m V002 (VNP14IMG)\n",
    "VIIRS/JPSS1 Active Fires 6-Min L2 Swath 375m V002 (VJ114IMG)\n",
    "\n",
    "Return: \n",
    "    - Downloaded/cloud access NetCDF granules for the above products\n",
    "    - GeoDataFrame representing active fire pixel locations and attributes (before geolocation)\n",
    "    - Geolocation grid representing pixel locations and overlap of adjacent orbits\n",
    "\n",
    "Author: maxwell.cook@colorado.edu\n",
    "\"\"\"\n",
    "\n",
    "import sys, os\n",
    "import earthaccess as ea\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import rioxarray as rxr\n",
    "import rasterio as rio\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "import datetime\n",
    "import traceback\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from rasterio.transform import from_bounds\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Custom functions\n",
    "sys.path.append(os.path.join(os.getcwd(),'code/'))\n",
    "from __functions import *\n",
    "        \n",
    "# Directories\n",
    "maindir = '/Users/max/Library/CloudStorage/OneDrive-Personal/mcook/'\n",
    "projdir = os.path.join(maindir, 'aspen-fire/Aim2/')\n",
    "\n",
    "# Output directories\n",
    "dataraw = os.path.join(projdir,'data/spatial/raw/VIIRS/')\n",
    "datamod = os.path.join(projdir,'data/spatial/mod/VIIRS/')\n",
    "\n",
    "print(\"Ready !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73b7ebd1-d1ba-400d-8b65-d3b993c033ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class and functions ready !\n"
     ]
    }
   ],
   "source": [
    "class Access_VIIRS_AFD:\n",
    "    \"\"\" \n",
    "    Accesses VIIRS Active Fire Data (AFD) within a region for given date range\n",
    "    \"\"\"\n",
    "    def __init__(self, start_date, last_date, geom = gpd.GeoDataFrame(),\n",
    "                 id_col='Fire_ID', name_col='Fire_Name',\n",
    "                 geog_crs = 'EPSG:4326', proj_crs = 'EPSG:5070',\n",
    "                 short_names = ['VNP14IMG', 'VJ114IMG'],\n",
    "                 buffer = None, out_directory=None, \n",
    "                 processed_granules=None, \n",
    "                 region=None\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            - start_date: the intial date for the granule search\n",
    "            - last_date: the final date for the granule search\n",
    "            - geom: GeoDataFrame for search request (fire perimeter)\n",
    "            - geog_crs: Geographic projection (to retrieve coordinate pairs in lat/lon)\n",
    "            - proj_crs: Projected coordinate system\n",
    "            - short_names: the granules to be downloaded\n",
    "            - buffer: Optional buffer for input geometry\n",
    "            - out_directory: output directory to store results\n",
    "            - download: If 'True', downloads the netcdf, otherwise processes in cloud\n",
    "        Returns:\n",
    "            - Downloaded files (VIIRS Active Fire Data NetCDF and Geolocation information)\n",
    "            - GeoDataFrame with non-geolocated (raw) fire detections\n",
    "        \"\"\"\n",
    "        # Extract coordinate bounds\n",
    "        if region is None:\n",
    "            # use the fire perimeter\n",
    "            self.coords, self.extent = get_coords(geom, buffer)\n",
    "            # print(f\"Bounding extent for data search: \\n{self.extent}\\n\")\n",
    "        elif region is not None and isinstance(region, gpd.GeoDataFrame):\n",
    "            # use the region boundary for FP and fire for search\n",
    "            _, self.extent = get_coords(region, buffer) # for extracting FP\n",
    "            self.coords, _ = get_coords(geom, buffer) # for data search\n",
    "            # print(f\"Bounding extent for data search: \\n{self.extent}\\n\")\n",
    "        else:\n",
    "            print(\"Input region is not a GeoDataFrame !!!\")\n",
    "            \n",
    "        # Extract class attributes\n",
    "        self.fire_id = geom[id_col].iloc[0]\n",
    "        self.fire_name = geom[name_col].iloc[0]\n",
    "        self.date_range = (str(start_date), str(last_date))\n",
    "        self.geog_crs = geog_crs\n",
    "        self.proj_crs = proj_crs\n",
    "        self.short_names = short_names\n",
    "        self.out_dir = out_directory\n",
    "        self.granule_log = os.path.join(dataraw, 'logs/vnp_vji_processed_granules.txt')\n",
    "        self.processed_granules = processed_granules\n",
    "        self.lut = pd.read_csv(os.path.join(projdir, 'data/tabular/raw/pix_size_lut.csv'))\n",
    "      \n",
    "    def ea_search_request(self):\n",
    "        \"\"\" Generate an earthaccess search request with the given parameters \"\"\"\n",
    "        query = ea.search_data(\n",
    "            short_name=self.short_names, \n",
    "            polygon=self.coords,\n",
    "            temporal=self.date_range, \n",
    "            cloud_hosted=True,\n",
    "            count=-1\n",
    "        )\n",
    "        \n",
    "        # Grab a list of granules from the search query\n",
    "        granules = [g['umm']['DataGranule']['Identifiers'][0]['Identifier'] for g in query]\n",
    "        N = len(granules)\n",
    "    \n",
    "        # Filter the query to only work with the \"new\" granules\n",
    "        # Skip if no new granules are required\n",
    "        if self.processed_granules is not None:\n",
    "            processed = [g.replace('.nc', '') for g in self.processed_granules]\n",
    "            new_granules = [g for g in granules if g not in processed]\n",
    "            if len(new_granules) == 0:\n",
    "                print(f\"\\t! All granules already processed, skipping ... !\")\n",
    "                return None, None\n",
    "            elif len(new_granules) > 0 and len(new_granules) < N:\n",
    "                print(f\"\\n\\t! Some granules already processed [{N - len(new_granules)}] !\")\n",
    "                query = [item for item in query if item['umm']['DataGranule']['Identifiers'][0]['Identifier'] in new_granules]\n",
    "                granules = [g['umm']['DataGranule']['Identifiers'][0]['Identifier'] for g in query]\n",
    "            else:\n",
    "                print(f\"\\n\\t! Starting processing for [{len(granules)}] granules !\")\n",
    "                query = query\n",
    "                granules = granules\n",
    "\n",
    "        # open the fileset\n",
    "        fileset = ea.open(query)\n",
    "\n",
    "        # return query results and list of granules\n",
    "        return fileset, granules\n",
    "             \n",
    "\n",
    "    def create_fire_gdf(self, fileset):\n",
    "        \"\"\" Creates a geodataframe with active fire detections from a directory with NetCDF files \"\"\"\n",
    "\n",
    "        granule_dfs = [] # to store the geolocated AFDs\n",
    "        \n",
    "        nprint = 10 # print counter\n",
    "        for fp in tqdm(fileset, desc=\"Processing granules\"):\n",
    "            df = pd.DataFrame() # to store the active fire data\n",
    "            with xr.open_dataset(fp, phony_dims='access') as swath:\n",
    "                # make sure there are fire pixels\n",
    "                if swath.FirePix == 0:\n",
    "                    continue\n",
    "                \n",
    "                # get the granule ID and associated geolocation swath\n",
    "                granule_id = swath.LocalGranuleID\n",
    "                geo_id = swath.VNP03IMG\n",
    "                \n",
    "                # get the data variables\n",
    "                lonfp = swath.variables['FP_longitude'][:] # fire pixel longitude\n",
    "                latfp = swath.variables['FP_latitude'][:] # fire pixel latitude\n",
    "                frp = swath.variables['FP_power'][:] # fire radiative power\n",
    "                t4 = swath.variables['FP_T4'][:] # I04 brightness temp (kelvins)\n",
    "                t5 = swath.variables['FP_T5'][:] # I05 brightness temp (kelvins)\n",
    "                m13 = swath.variables['FP_Rad13'][:] # M13 radiance (kelvin)\n",
    "                sample = swath.variables['FP_sample'][:]\n",
    "                line = swath.variables['FP_line'][:]\n",
    "                # get the fire mask for fire pixels\n",
    "                fire_mask = swath['fire mask'][line, sample].values\n",
    "\n",
    "            # gather information from file name\n",
    "            timestamp = granule_id.split('.')[1:3]\n",
    "            year = timestamp[0][1:5]\n",
    "            day = timestamp[0][5:8]\n",
    "            acqtime = timestamp[1]\n",
    "            acqdate = dt.datetime.strptime(year+day, '%Y%j').strftime('%-m/%-d/%Y')\n",
    "            \n",
    "            df['longitude'] = lonfp\n",
    "            df['latitude'] = latfp\n",
    "            df['j'] = sample #sample number for pixel size lookup\n",
    "            df['fire_mask'] = fire_mask\n",
    "            df['confidence'] = pd.Categorical(df.fire_mask)\n",
    "            df.confidence = df.confidence.replace(\n",
    "                {0:'x', 1:'x', 2:'x', 3:'x', 4:'x', 5:'x', 6:'x', 7:'l', 8:'n', 9:'h'})\n",
    "            df['frp'] = frp\n",
    "            df['t4'] = t4\n",
    "            df['t5'] = t5\n",
    "            df['m13'] = m13\n",
    "            df['acq_date'] = acqdate\n",
    "            df['acq_time'] = acqtime\n",
    "            df['daynight'] = swath.DayNightFlag\n",
    "            df['satellite'] = swath.PlatformShortName\n",
    "            df['short_name'] = swath.ShortName\n",
    "            df['granule_id'] = granule_id\n",
    "            df['geo_id'] = geo_id\n",
    "        \n",
    "            df = pd.merge(df, self.lut, left_on='j', right_on='sample', how='left')\n",
    "            df.drop(columns=['j'], inplace=True)\n",
    "            \n",
    "            granule_dfs.append(df) # append the granule dataframe\n",
    "\n",
    "            # write the granule id to the log file\n",
    "            with open(self.granule_log, 'a') as log_file:\n",
    "                log_file.write(f\"{granule_id}\\n\")\n",
    "\n",
    "            # save a csv file\n",
    "            out_dir = os.path.join(dataraw,\"granules/\")\n",
    "            if not os.path.exists(out_dir):\n",
    "                os.makedirs(out_dir)\n",
    "            df.to_csv(os.path.join(out_dir,f\"{granule_id[:-3]}.csv\"))\n",
    "\n",
    "        gc.collect() # clear out garbage\n",
    "        \n",
    "        # concatenate the out dfs\n",
    "        if len(granule_dfs) > 0:\n",
    "            fire_data = pd.concat(granule_dfs) # for the entire list of granules\n",
    "            return fire_data\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "print(\"Class and functions ready !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d1db138-d9da-4606-a1fe-7154b40c17aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available attributes: \n",
      "Index(['Fire_ID', 'Fire_Name', 'Final_Acres', 'Source', 'Aspen_Pct',\n",
      "       'INCIDENT_ID', 'INCIDENT_NAME', 'START_YEAR', 'ICS_ACRES', 'CAUSE',\n",
      "       'DISCOVERY_DATE', 'DISCOVERY_DOY', 'WF_CESSATION_DATE',\n",
      "       'WF_CESSATION_DOY', 'STR_DESTROYED_TOTAL', 'STR_DAMAGED_TOTAL',\n",
      "       'STR_THREATENED_MAX', 'EVACUATION_REPORTED', 'PEAK_EVACUATIONS',\n",
      "       'WF_PEAK_AERIAL', 'WF_PEAK_PERSONNEL', 'na_l3name', 'geometry',\n",
      "       'start_day', 'last_day'],\n",
      "      dtype='object')\n",
      "\n",
      "There are [115] fires.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fire_Name</th>\n",
       "      <th>start_day</th>\n",
       "      <th>DISCOVERY_DATE</th>\n",
       "      <th>WF_CESSATION_DATE</th>\n",
       "      <th>last_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CALF CANYON-HERMITS PEAK</td>\n",
       "      <td>2022-03-23</td>\n",
       "      <td>2022-04-06</td>\n",
       "      <td>2022-06-14</td>\n",
       "      <td>2022-06-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAMERON PEAK</td>\n",
       "      <td>2020-07-30</td>\n",
       "      <td>2020-08-13</td>\n",
       "      <td>2020-10-17</td>\n",
       "      <td>2020-10-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EAST TROUBLESOME</td>\n",
       "      <td>2020-09-30</td>\n",
       "      <td>2020-10-14</td>\n",
       "      <td>2020-10-23</td>\n",
       "      <td>2020-11-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MULLEN</td>\n",
       "      <td>2020-09-03</td>\n",
       "      <td>2020-09-17</td>\n",
       "      <td>2020-10-08</td>\n",
       "      <td>2020-10-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SPRING CREEK</td>\n",
       "      <td>2018-06-13</td>\n",
       "      <td>2018-06-27</td>\n",
       "      <td>2018-07-05</td>\n",
       "      <td>2018-07-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Fire_Name   start_day DISCOVERY_DATE WF_CESSATION_DATE  \\\n",
       "0  CALF CANYON-HERMITS PEAK  2022-03-23     2022-04-06        2022-06-14   \n",
       "1              CAMERON PEAK  2020-07-30     2020-08-13        2020-10-17   \n",
       "2          EAST TROUBLESOME  2020-09-30     2020-10-14        2020-10-23   \n",
       "3                    MULLEN  2020-09-03     2020-09-17        2020-10-08   \n",
       "4              SPRING CREEK  2018-06-13     2018-06-27        2018-07-05   \n",
       "\n",
       "     last_day  \n",
       "0  2022-06-28  \n",
       "1  2020-10-31  \n",
       "2  2020-11-06  \n",
       "3  2020-10-22  \n",
       "4  2018-07-19  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the fire dataset for the Southern Rockies\n",
    "fp = os.path.join(projdir,'data/spatial/mod/srm_fire_census_2017_to_2023_ics_bounds.gpkg')\n",
    "fires = gpd.read_file(fp)\n",
    "\n",
    "# tidy the date columns\n",
    "fires['DISCOVERY_DATE'] = pd.to_datetime(fires['DISCOVERY_DATE']).dt.date\n",
    "fires['WF_CESSATION_DATE'] = pd.to_datetime(fires['WF_CESSATION_DATE']).dt.date\n",
    "fires['start_day'] = fires['DISCOVERY_DATE'] - pd.to_timedelta(14, unit='d')\n",
    "fires['last_day'] = fires['WF_CESSATION_DATE'] + pd.to_timedelta(14, unit='d')\n",
    "fires['Fire_Name'] = fires['Fire_Name'].fillna(fires['INCIDENT_NAME'])\n",
    "\n",
    "print(f\"Available attributes: \\n{fires.columns}\")\n",
    "print(f\"\\nThere are [{len(fires)}] fires.\")\n",
    "fires[['Fire_Name','start_day','DISCOVERY_DATE','WF_CESSATION_DATE','last_day']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8af0edb-153d-45e8-9d84-35086aa3215c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2018-07-12    16\n",
      "2018-07-16    16\n",
      "2018-07-08    16\n",
      "2018-07-09    16\n",
      "2018-07-10    16\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with individual dates for each fire\n",
    "date_counts = pd.DataFrame(\n",
    "    [(fire['Fire_ID'], single_date)\n",
    "     for _, fire in fires.iterrows()\n",
    "     for single_date in pd.date_range(fire['start_day'], fire['last_day'])],\n",
    "    columns=['Fire_ID', 'Date']\n",
    ")['Date'].value_counts()\n",
    "print(date_counts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014fb18e-1db4-4def-aedd-8b833b51e26b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c92ac8f-4196-4e9f-a815-b7e45965d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run for fire perimeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "070823f2-cc03-4408-8b58-3467a1d4e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already processed [7915] granules.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for already processed granules\n",
    "granule_log = os.path.join(dataraw, 'logs/vnp_vji_processed_granules.txt')\n",
    "if os.path.exists(granule_log):\n",
    "    with open(granule_log, 'r') as log_file:\n",
    "        granules_p = set([line.strip() for line in log_file.readlines()])\n",
    "else:\n",
    "    granules_p = set()\n",
    "\n",
    "print(f\"Already processed [{len(granules_p)}] granules.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c72c3128-d09b-4523-a0e9-bf90a63b5bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing for 56_FIRE fire:\n",
      "Granules found: 90\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for HONDITO fire:\n",
      "Granules found: 98\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for BONITA fire:\n",
      "Granules found: 133\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for CAJETE fire:\n",
      "Granules found: 89\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for WHITMAN fire:\n",
      "Granules found: 88\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for MILL_CREEK fire:\n",
      "Granules found: 104\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for GUTZLER fire:\n",
      "Granules found: 95\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for KEYSTONE fire:\n",
      "Granules found: 125\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for OJITOS fire:\n",
      "Granules found: 307\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for PEGGY fire:\n",
      "Granules found: 118\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for ALAMEDA fire:\n",
      "Granules found: 95\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for EIGHT_FOUR_TWO fire:\n",
      "Granules found: 120\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for BIG_RED fire:\n",
      "Granules found: 152\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for Himes_Peak fire:\n",
      "Granules found: 106\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for DEEP_CREEK fire:\n",
      "Granules found: 103\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for DRAW fire:\n",
      "Granules found: 107\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for PALMER fire:\n",
      "Granules found: 120\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for DEER_CREEK fire:\n",
      "Granules found: 112\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for JOHNSON_RIDGE fire:\n",
      "Granules found: 89\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for UTE_PARK fire:\n",
      "Granules found: 182\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for 416 fire:\n",
      "Granules found: 355\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for BURRO fire:\n",
      "Granules found: 293\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for BOCCO fire:\n",
      "Granules found: 196\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for BADGER_CREEK fire:\n",
      "Granules found: 231\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for SARDINAS_CANYON fire:\n",
      "Granules found: 211\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for SPRING_CREEK fire:\n",
      "Granules found: 220\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for SUGARLOAF fire:\n",
      "Granules found: 177\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for WESTON_PASS fire:\n",
      "Granules found: 212\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for CHATEAU fire:\n",
      "Granules found: 175\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for MORRIS_CREEK fire:\n",
      "Granules found: 194\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for LAKE_CHRISTINE fire:\n",
      "Granules found: 301\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for SARCA fire:\n",
      "Granules found: 383\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for ARAGON fire:\n",
      "Granules found: 235\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for TARANTULA fire:\n",
      "Granules found: 172\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for SILVER_CREEK fire:\n",
      "Granules found: 629\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for VENADO fire:\n",
      "Granules found: 208\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for PLATEAU fire:\n",
      "Granules found: 324\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for WEST_GUARD fire:\n",
      "Granules found: 189\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for CACHE_CREEK fire:\n",
      "Granules found: 293\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for BULL_DRAW fire:\n",
      "Granules found: 522\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for CABIN_LAKE fire:\n",
      "Granules found: 281\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for Turner fire:\n",
      "Granules found: 221\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for BRITANIA_MOUNTAIN fire:\n",
      "Granules found: 232\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for HORSE fire:\n",
      "Granules found: 292\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for SEAMAN fire:\n",
      "Granules found: 185\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for RYAN fire:\n",
      "Granules found: 277\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for RYAN fire:\n",
      "Granules found: 277\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for REVEILLE fire:\n",
      "Granules found: 392\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for GURULE fire:\n",
      "Granules found: 235\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for BEAVER fire:\n",
      "Granules found: 236\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for NARANJO fire:\n",
      "Granules found: 214\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for FRANCISQUITO fire:\n",
      "Granules found: 198\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for 441 fire:\n",
      "Granules found: 236\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for MIDDLE_MAMM fire:\n",
      "Granules found: 656\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for CONEJOS fire:\n",
      "Granules found: 257\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for CUEVA fire:\n",
      "Granules found: 298\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for INDIAN_RUN fire:\n",
      "Granules found: 270\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for MONTOYA_SPRINGS fire:\n",
      "Granules found: 218\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for AMOLE fire:\n",
      "Granules found: 226\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for DECKER fire:\n",
      "Granules found: 402\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for BRUSH_CREEK fire:\n",
      "Granules found: 182\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for GRANITE_LAKE fire:\n",
      "Granules found: 224\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for COW_CREEK fire:\n",
      "Granules found: 197\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for SAND_CREEK fire:\n",
      "Granules found: 276\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for GOOSE_CREEK fire:\n",
      "Granules found: 227\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for YMCA fire:\n",
      "Granules found: 176\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for GRIZZLY_CREEK fire:\n",
      "Granules found: 263\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for CAMERON_PEAK fire:\n",
      "Granules found: 599\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for WILLIAMS_FORK fire:\n",
      "Granules found: 531\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for MEDIO fire:\n",
      "Granules found: 279\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for Lewstone fire:\n",
      "Granules found: 179\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for THORPE fire:\n",
      "Granules found: 173\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for MIDDLE_FORK fire:\n",
      "Granules found: 519\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for MULLEN fire:\n",
      "Granules found: 315\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for EAST_TROUBLESOME fire:\n",
      "Granules found: 231\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for CALWOOD fire:\n",
      "Granules found: 190\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for LUNA fire:\n",
      "Granules found: 198\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for LEFTHAND fire:\n",
      "Granules found: 180\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for ICE fire:\n",
      "Granules found: 173\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for POISON_SPRINGS fire:\n",
      "Granules found: 186\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for CUERVITO fire:\n",
      "Granules found: 235\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for WOLF_DRAW fire:\n",
      "Granules found: 172\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for POSO fire:\n",
      "Granules found: 266\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for PACK_CREEK fire:\n",
      "Granules found: 209\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for RINCON fire:\n",
      "Granules found: 189\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for SYLVAN fire:\n",
      "Granules found: 194\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for MUDDY_SLIDE fire:\n",
      "Granules found: 190\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for MORGAN_CREEK fire:\n",
      "Granules found: 301\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for AMARGO fire:\n",
      "Granules found: 221\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for BLACK_MOUNTAIN fire:\n",
      "Granules found: 192\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for KRUGER_MOUNTAIN fire:\n",
      "Granules found: 172\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for CALF_CANYON-HERMITS_PEAK fire:\n",
      "Granules found: 584\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for COOKS_PEAK fire:\n",
      "Granules found: 240\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for CERRO_PELADO fire:\n",
      "Granules found: 272\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for HIGH_PARK fire:\n",
      "Granules found: 190\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for PLUMTAW fire:\n",
      "Granules found: 172\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for PERINS_PEAK fire:\n",
      "Granules found: 171\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for MENKHAVEN fire:\n",
      "Granules found: 174\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for MIDNIGHT fire:\n",
      "Granules found: 191\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for MONDAY_CREEK fire:\n",
      "Granules found: 177\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for SUGARLOAF fire:\n",
      "Granules found: 183\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for CR_21 fire:\n",
      "Granules found: 184\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for 403 fire:\n",
      "Granules found: 191\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for COMANCHE fire:\n",
      "Granules found: 255\n",
      "\n",
      "\t! Some granules already processed [147] !\n",
      "Opening 108 granules, approx size: 0.23 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f122ca66dc24cf385f1cc6032d3d81c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53a29acd9194a8289ed55613fb4cf2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88712efe3086423694f29ceb06539858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tExtracting active fires ...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db232d74d3b4aa8bda3fe315c3fb8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing granules:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total elapsed time for 67    COMANCHE\n",
      "Name: Fire_Name, dtype: object: 3.11 minutes.\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Processing for CHRIS_MOUNTAIN fire:\n",
      "Granules found: 200\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for LOWLINE fire:\n",
      "Granules found: 392\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for BEAR_CREEK fire:\n",
      "Granules found: 421\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for DRY_LAKE fire:\n",
      "Granules found: 188\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for QUARTZ_RIDGE fire:\n",
      "Granules found: 370\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for BLACK_FEATHER fire:\n",
      "Granules found: 170\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for AMERICAN_MESA fire:\n",
      "Granules found: 156\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for EL_VALLE fire:\n",
      "Granules found: 168\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for HOPE fire:\n",
      "Granules found: 206\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for TRAIL_SPRINGS fire:\n",
      "Granules found: 274\n",
      "\t! All granules already processed, skipping ... !\n",
      "Processing for MILL_CREEK_2 fire:\n",
      "Granules found: 407\n",
      "\t! All granules already processed, skipping ... !\n",
      "\n",
      "Total elapsed time: 6.23 minutes.\n",
      "\n",
      "\n",
      "~~~~~~~~~~\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()   \n",
    "\n",
    "# Get a list of fire IDs sorted by ignition date\n",
    "fires = fires.sort_values(by=['START_YEAR','start_day'])\n",
    "fire_ids = fires['Fire_ID'].unique()\n",
    "\n",
    "afd_dfs = [] # to store the output geodataframes\n",
    "\n",
    "# Loop fire ids\n",
    "for fire_id in fire_ids:\n",
    "    t00 = time.time()\n",
    "\n",
    "    fire = fires[fires['Fire_ID'] == fire_id]\n",
    "    fire_name = fire['Fire_Name'].iloc[0]\n",
    "    fire_name = fire_name.replace(\" \", \"_\")\n",
    "    print(f\"Processing for {fire_name} fire:\")\n",
    "    \n",
    "    da_access = Access_VIIRS_AFD(\n",
    "        start_date=fire['start_day'].iloc[0],\n",
    "        last_date=fire['last_day'].iloc[0],\n",
    "        geom=fire,\n",
    "        buffer=1000,\n",
    "        short_names=['VNP14IMG','VJ114IMG'],\n",
    "        out_directory=dataraw,\n",
    "        processed_granules=granules_p\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        fileset, granules = da_access.ea_search_request()\n",
    "\n",
    "        if granules is None:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n\\tExtracting active fires ...\\n\")\n",
    "        afd_fire = da_access.create_fire_gdf(fileset)\n",
    "\n",
    "        # save the progress so far\n",
    "        if afd_fire is not None:\n",
    "            afd_dfs.append(afd_fire)\n",
    "            granules_p.update(granules) # running list\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nSkipping fire {fire['Fire_Name']}\\n{e}\\n\")\n",
    "        traceback.print_exc()  # This will print the full traceback\n",
    "        continue # continue to the next fire id\n",
    "\n",
    "    t1 = (time.time() - t00) / 60\n",
    "    print(f\"\\nTotal elapsed time for {fire['Fire_Name']}: {t1:.2f} minutes.\")\n",
    "    print(\"\\n~~~~~~~~~~\\n\")\n",
    "\n",
    "t2 = (time.time() - t0) / 60\n",
    "print(f\"\\nTotal elapsed time: {t2:.2f} minutes.\\n\")\n",
    "print(\"\\n~~~~~~~~~~\\n\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16049c96-23fe-411a-ae8e-ee111842b311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8023\n"
     ]
    }
   ],
   "source": [
    "granules_ = glob.glob(os.path.join(dataraw,'granules/*.csv'))\n",
    "print(len(granules_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ecdf797-cd61-4cd6-91d1-a355a4eb4956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>fire_mask</th>\n",
       "      <th>confidence</th>\n",
       "      <th>frp</th>\n",
       "      <th>t4</th>\n",
       "      <th>t5</th>\n",
       "      <th>m13</th>\n",
       "      <th>acq_date</th>\n",
       "      <th>...</th>\n",
       "      <th>daynight</th>\n",
       "      <th>satellite</th>\n",
       "      <th>short_name</th>\n",
       "      <th>granule_id</th>\n",
       "      <th>geo_id</th>\n",
       "      <th>sample</th>\n",
       "      <th>along_scan</th>\n",
       "      <th>along_track</th>\n",
       "      <th>scan_angle</th>\n",
       "      <th>pix_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-95.160160</td>\n",
       "      <td>34.074640</td>\n",
       "      <td>9</td>\n",
       "      <td>h</td>\n",
       "      <td>14.550305</td>\n",
       "      <td>367.00000</td>\n",
       "      <td>295.69968</td>\n",
       "      <td>2.373925</td>\n",
       "      <td>6/27/2019</td>\n",
       "      <td>...</td>\n",
       "      <td>Day</td>\n",
       "      <td>JPSS-1</td>\n",
       "      <td>VJ114IMG</td>\n",
       "      <td>VJ114IMG.A2019178.2006.002.2024029081304.nc</td>\n",
       "      <td>VJ103IMG.A2019178.2006.021.2021049184623.nc</td>\n",
       "      <td>924</td>\n",
       "      <td>0.381910</td>\n",
       "      <td>0.588271</td>\n",
       "      <td>47.8431</td>\n",
       "      <td>0.224667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-95.164330</td>\n",
       "      <td>34.074383</td>\n",
       "      <td>8</td>\n",
       "      <td>n</td>\n",
       "      <td>14.550305</td>\n",
       "      <td>350.57697</td>\n",
       "      <td>296.18445</td>\n",
       "      <td>2.373925</td>\n",
       "      <td>6/27/2019</td>\n",
       "      <td>...</td>\n",
       "      <td>Day</td>\n",
       "      <td>JPSS-1</td>\n",
       "      <td>VJ114IMG</td>\n",
       "      <td>VJ114IMG.A2019178.2006.002.2024029081304.nc</td>\n",
       "      <td>VJ103IMG.A2019178.2006.021.2021049184623.nc</td>\n",
       "      <td>925</td>\n",
       "      <td>0.381694</td>\n",
       "      <td>0.588131</td>\n",
       "      <td>47.8342</td>\n",
       "      <td>0.224486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-95.160630</td>\n",
       "      <td>34.080082</td>\n",
       "      <td>8</td>\n",
       "      <td>n</td>\n",
       "      <td>6.089355</td>\n",
       "      <td>343.91302</td>\n",
       "      <td>296.12167</td>\n",
       "      <td>1.406672</td>\n",
       "      <td>6/27/2019</td>\n",
       "      <td>...</td>\n",
       "      <td>Day</td>\n",
       "      <td>JPSS-1</td>\n",
       "      <td>VJ114IMG</td>\n",
       "      <td>VJ114IMG.A2019178.2006.002.2024029081304.nc</td>\n",
       "      <td>VJ103IMG.A2019178.2006.021.2021049184623.nc</td>\n",
       "      <td>924</td>\n",
       "      <td>0.381910</td>\n",
       "      <td>0.588271</td>\n",
       "      <td>47.8431</td>\n",
       "      <td>0.224667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-95.164825</td>\n",
       "      <td>34.079823</td>\n",
       "      <td>8</td>\n",
       "      <td>n</td>\n",
       "      <td>6.089355</td>\n",
       "      <td>345.46840</td>\n",
       "      <td>296.05000</td>\n",
       "      <td>1.406672</td>\n",
       "      <td>6/27/2019</td>\n",
       "      <td>...</td>\n",
       "      <td>Day</td>\n",
       "      <td>JPSS-1</td>\n",
       "      <td>VJ114IMG</td>\n",
       "      <td>VJ114IMG.A2019178.2006.002.2024029081304.nc</td>\n",
       "      <td>VJ103IMG.A2019178.2006.021.2021049184623.nc</td>\n",
       "      <td>925</td>\n",
       "      <td>0.381694</td>\n",
       "      <td>0.588131</td>\n",
       "      <td>47.8342</td>\n",
       "      <td>0.224486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-95.906006</td>\n",
       "      <td>34.813230</td>\n",
       "      <td>8</td>\n",
       "      <td>n</td>\n",
       "      <td>4.297072</td>\n",
       "      <td>340.28574</td>\n",
       "      <td>296.18735</td>\n",
       "      <td>1.041121</td>\n",
       "      <td>6/27/2019</td>\n",
       "      <td>...</td>\n",
       "      <td>Day</td>\n",
       "      <td>JPSS-1</td>\n",
       "      <td>VJ114IMG</td>\n",
       "      <td>VJ114IMG.A2019178.2006.002.2024029081304.nc</td>\n",
       "      <td>VJ103IMG.A2019178.2006.021.2021049184623.nc</td>\n",
       "      <td>1078</td>\n",
       "      <td>0.351243</td>\n",
       "      <td>0.567857</td>\n",
       "      <td>46.4736</td>\n",
       "      <td>0.199456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  longitude   latitude  fire_mask confidence        frp  \\\n",
       "0           0 -95.160160  34.074640          9          h  14.550305   \n",
       "1           1 -95.164330  34.074383          8          n  14.550305   \n",
       "2           2 -95.160630  34.080082          8          n   6.089355   \n",
       "3           3 -95.164825  34.079823          8          n   6.089355   \n",
       "4           4 -95.906006  34.813230          8          n   4.297072   \n",
       "\n",
       "          t4         t5       m13   acq_date  ...  daynight satellite  \\\n",
       "0  367.00000  295.69968  2.373925  6/27/2019  ...       Day    JPSS-1   \n",
       "1  350.57697  296.18445  2.373925  6/27/2019  ...       Day    JPSS-1   \n",
       "2  343.91302  296.12167  1.406672  6/27/2019  ...       Day    JPSS-1   \n",
       "3  345.46840  296.05000  1.406672  6/27/2019  ...       Day    JPSS-1   \n",
       "4  340.28574  296.18735  1.041121  6/27/2019  ...       Day    JPSS-1   \n",
       "\n",
       "  short_name                                   granule_id  \\\n",
       "0   VJ114IMG  VJ114IMG.A2019178.2006.002.2024029081304.nc   \n",
       "1   VJ114IMG  VJ114IMG.A2019178.2006.002.2024029081304.nc   \n",
       "2   VJ114IMG  VJ114IMG.A2019178.2006.002.2024029081304.nc   \n",
       "3   VJ114IMG  VJ114IMG.A2019178.2006.002.2024029081304.nc   \n",
       "4   VJ114IMG  VJ114IMG.A2019178.2006.002.2024029081304.nc   \n",
       "\n",
       "                                        geo_id sample  along_scan  \\\n",
       "0  VJ103IMG.A2019178.2006.021.2021049184623.nc    924    0.381910   \n",
       "1  VJ103IMG.A2019178.2006.021.2021049184623.nc    925    0.381694   \n",
       "2  VJ103IMG.A2019178.2006.021.2021049184623.nc    924    0.381910   \n",
       "3  VJ103IMG.A2019178.2006.021.2021049184623.nc    925    0.381694   \n",
       "4  VJ103IMG.A2019178.2006.021.2021049184623.nc   1078    0.351243   \n",
       "\n",
       "   along_track  scan_angle  pix_area  \n",
       "0     0.588271     47.8431  0.224667  \n",
       "1     0.588131     47.8342  0.224486  \n",
       "2     0.588271     47.8431  0.224667  \n",
       "3     0.588131     47.8342  0.224486  \n",
       "4     0.567857     46.4736  0.199456  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afds = pd.concat((pd.read_csv(f) for f in granules_), ignore_index=True)\n",
    "afds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "368a14c6-b79b-4f64-9165-5a7fd743460a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3639128"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(afds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b83e6b4-89c7-43d7-b56a-336ce457f339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['JPSS-1', 'SUOMI-NPP'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afds['satellite'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84fc788b-23eb-4c01-81f0-6efbc4091b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: /Users/max/Library/CloudStorage/OneDrive-Personal/mcook/aspen-fire/Aim2/data/spatial/raw/VIIRS/viirs_snpp_jpss1_afd_.csv\n"
     ]
    }
   ],
   "source": [
    "# save the file.\n",
    "out_fp = os.path.join(dataraw, f'viirs_snpp_jpss1_afd_.csv')\n",
    "afds.to_csv(out_fp)\n",
    "print(f\"Saved to: {out_fp}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aspen-fire",
   "language": "python",
   "name": "aspen-fire"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
